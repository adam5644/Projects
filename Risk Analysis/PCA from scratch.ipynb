{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read in raw data file\n",
    "data_fn = \"sofr_hist_rate_chg.csv\"\n",
    "\n",
    "df_data = pd.read_csv(data_fn)\n",
    "arr_data = df_data.to_numpy()\n",
    "tenor_labels = list(df_data.columns)\n",
    "# curve rate tenors in year fractions\n",
    "tenorInYears = [0.002778, 0.083333, 0.166667, 0.25, 0.50, 0.75, 1, 2, 3, 4, \n",
    "               5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n",
    "              20, 25, 30, 35, 40]\n",
    "#tenorInYears = [1, 2, 3, 4, \n",
    "#                5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n",
    " #               20, 25, 30, 35, 40]\n",
    "\n",
    "\n",
    "\n",
    "# standardize each column data\n",
    "arr_data_stdized = StandardScaler().fit_transform(df_data) # mean0, var1 #mathematically, (x-mean)/vol\n",
    "df_data_stdized = pd.DataFrame(arr_data_stdized, columns = tenor_labels)\n",
    "\n",
    "\n",
    "##################################\n",
    "####### PCA construction #########\n",
    "##################################\n",
    "\n",
    "# step (1): compute the correlation matrix\n",
    "n = len(arr_data_stdized)\n",
    "mat_corr = np.dot(arr_data_stdized.transpose(),arr_data_stdized)/n # corr_matrix = (1/n) X.T * X where X is standardised data matrix\n",
    "\n",
    "# step (2): eigen-decompose the correlation matrix using numpy function which\n",
    "#           automatically orders the eigen vectors by the size of the assocaited \n",
    "#           eigenvalues\n",
    "egval, egvec = np.linalg.eig(mat_corr)\n",
    "\n",
    "# visualize the PCs (recall the PCs are effectively the eigenvectors)\n",
    "pc_labels = [\"PC1\" , \"PC2\" , \"PC3\" , \"PC4\" , \"PC5\" , \"PC6\" , \"PC7\" , \n",
    "             \"PC8\" , \"PC9\" , \"PC10\" , \"PC11\" , \"PC12\" , \"PC13\" , \"PC14\" , \n",
    "            \"PC15\" , \"PC16\" , \"PC17\" , \"PC18\" , \"PC19\" , \"PC20\" , \"PC21\" , \n",
    "           \"PC22\" , \"PC23\" , \"PC24\" , \"PC25\" , \"PC26\" , \"PC27\" , \"PC28\" , \n",
    "          \"PC29\", \"PC30\"]\n",
    "#pc_labels = [\"PC1\" , \"PC2\" , \"PC3\" , \"PC4\" , \"PC5\" , \"PC6\" , \"PC7\" , \n",
    " #            \"PC8\" , \"PC9\" , \"PC10\" , \"PC11\" , \"PC12\" , \"PC13\" , \"PC14\" , \n",
    "  #           \"PC15\" , \"PC16\" , \"PC17\" , \"PC18\" , \"PC19\" , \"PC20\" , \"PC21\" , \n",
    "   #          \"PC22\" , \"PC23\" , \"PC24\"]\n",
    "df_pcs =  pd.DataFrame(egvec, columns = pc_labels)\n",
    "plt.plot(tenorInYears, df_pcs[\"PC1\"], label = \"PC1\")\n",
    "plt.plot(tenorInYears, df_pcs[\"PC2\"], label = \"PC2\")\n",
    "plt.plot(tenorInYears, df_pcs[\"PC3\"], label = \"PC3\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# calculate projected PC coordinates of the standardized data\n",
    "pc_coordinates = np.dot(arr_data_stdized, egvec)\n",
    "\n",
    "# data reconstruction\n",
    "\n",
    "# cache the mean and standard deviation of the original data \n",
    "# for reverting the data standardization\n",
    "mat_mean = np.tile(np.mean(arr_data, axis = 0), (n, 1))\n",
    "stdev = np.std(arr_data, axis = 0)\n",
    "mat_stdev = np.diag(stdev)\n",
    "\n",
    "#1) full reconstruction\n",
    "data_stdized_full_reconstruction = np.dot(pc_coordinates, egvec.transpose())                                        \n",
    "data_full_reconstruction = np.add(mat_mean, np.dot(data_stdized_full_reconstruction, mat_stdev)) # reconstructed = (PC . eigenvectors_T).vol + mean\n",
    "#check reconstruction success/failure:\n",
    "#print(\"min difference: \" + str(np.min(data_full_reconstruction - arr_data)))\n",
    "#print(\"max difference: \" + str(np.max(data_full_reconstruction - arr_data)))\n",
    "\n",
    "\n",
    "#2) compression (utilizing the first 5 PCs)\n",
    "data_stdized_comp = np.dot(pc_coordinates[:,0:15], egvec[:,0:15].transpose())\n",
    "data_comp = np.add(mat_mean, np.dot(data_stdized_comp, mat_stdev))\n",
    "# revert data standardization to obtained the raw curve rate data approximated\n",
    "# by the first 5 PCs\n",
    "\n",
    "# compare approximated curve on the i-th date using the first 5 PCs to the original curve\n",
    "i = 1\n",
    "plt.plot(tenorInYears, arr_data[i], label = \"original curve\")\n",
    "plt.plot(tenorInYears, data_comp[i], label = \"approximated curve (based on PC1-5)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
